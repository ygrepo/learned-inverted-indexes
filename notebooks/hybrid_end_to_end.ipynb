{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torch_data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import local dependencies\n",
    "import sys\n",
    "sys.path.insert(0, \"../src\")\n",
    "from collection import Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch_data.Dataset):\n",
    "    def __init__(self, data_list, pos_list):\n",
    "        self.data = data_list\n",
    "        self.pos = pos_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.pos[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_tensors(data):\n",
    "    tensor_list = []\n",
    "    for pl in data:\n",
    "        tensor_list.append(torch.tensor(pl, dtype=torch.float64))\n",
    "    return tensor_list\n",
    "\n",
    "\n",
    "def load_test_data(posting_length_to_use):\n",
    "    # Load data\n",
    "    test_collection = Collection(\"../test_data/test_collection\")\n",
    "    posting_lists = []\n",
    "    for _, pl in enumerate(test_collection):\n",
    "        if len(pl[0]) >= posting_length_to_use:\n",
    "            posting_lists.append(np.array(pl[0], dtype=np.int))\n",
    "    #posting_lists.sort(key=lambda x: np.shape(x)[0], reverse=True)\n",
    "\n",
    "    data = list_of_tensors(posting_lists)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, num_hidden, hidden_size):\n",
    "        \"\"\"\n",
    "        num_hidden: number of hidden layers\n",
    "        hidden_size: a list of the sizes (num of neurons) of the hidden layers\n",
    "        \"\"\"\n",
    "        super(Network, self).__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.fc = []    # fc layers\n",
    "        self.relu = []  # ReLU activations\n",
    "        input_size = 1  # size of the previous layer (input of current layer)\n",
    "        for fc_idx in range(num_hidden):\n",
    "            # add fc layer and ReLU activation\n",
    "            self.fc.append(nn.Linear(input_size, hidden_size[fc_idx]))\n",
    "            self.relu.append(nn.ReLU())\n",
    "            # input of next layer should be output of this layer\n",
    "            input_size = hidden_size[fc_idx]\n",
    "        # the layer should always have 1-dim output\n",
    "        self.last = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for fc_idx in range(self.num_hidden):\n",
    "            out = self.fc[fc_idx](out)\n",
    "            out = self.relu[fc_idx](out)\n",
    "        out = self.last(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3836\n",
      "torch.Size([2021])\n"
     ]
    }
   ],
   "source": [
    "data = load_test_data(128)\n",
    "print(len(data))\n",
    "pl = data[0]\n",
    "print(pl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_pl = len(pl)\n",
    "pos_list = torch.arange(len_pl, dtype=torch.float64).to(device)\n",
    "data_list = pl\n",
    "ds = [[Dataset(data_list, pos_list)]]\n",
    "#ds = [[Dataset(pos_list, data_list)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_source):\n",
    "    \"\"\"\n",
    "    load data in csv file\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    pos = []\n",
    "    with open(csv_source) as ds:\n",
    "        for line in ds:\n",
    "            dp = line.rstrip().split(',')\n",
    "            # each data/index should be a 1x1 PyTorch tensor\n",
    "            data.append(torch.tensor([float(dp[0])]))\n",
    "            pos.append(torch.tensor([float(dp[1])]))\n",
    "    return data, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, pos = load_data(\"random.csv\")\n",
    "ds = [[Dataset(data, pos)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max of index (position), used to determine next-stage model\n",
    "max_pos = len(data)\n",
    "#max_pos = len(pl)\n",
    "#max_pos = pl[-1]\n",
    "\n",
    "# model is a 2-dim list, entry i, j is the model for stage i, model j\n",
    "models = []\n",
    "\n",
    "# num_model is a tuple, entry i is the number of models for stage i\n",
    "#num_model = [1]\n",
    "num_model = (1, 10)\n",
    "#num_model = (1, 10, 10)\n",
    "#num_model = (1, 10, 10)\n",
    "\n",
    "# model_params is a tuple, entry i is the params of models in stage i\n",
    "# each entry specifies (num of hidden layers, size of each hidden layer)\n",
    "#model_params = ((2, [2000, 67]), (2, [1000, 500]))\n",
    "#model_params = [(3, [4, 8, 8])]\n",
    "model_params = ((2, [4, 8]), (2, [4, 8]))\n",
    "#model_params = [(1, [4])]\n",
    "#model_params = [(2, [4, 8]), (2, [4, 8]),(2, [4, 8])]\n",
    "#print(model_params[0][0])\n",
    "# number of stages\n",
    "num_stage = len(num_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage=0, Model=0, 100000 data points\n",
      "Loss: 81.91110741190667\n",
      "Loss: 79.11101674089831\n",
      "Loss: 77.1803571456556\n",
      "Loss: 81.78773118638419\n",
      "Stage=1, Model=0, 10066 data points\n",
      "Loss: 518.732087817916\n",
      "Loss: 27.021529306656213\n",
      "Loss: 26.91156672758241\n",
      "Loss: 26.910463223642182\n",
      "Stage=1, Model=1, 9886 data points\n",
      "Loss: 3572097.717160348\n",
      "Loss: 3470537.310600406\n",
      "Loss: 3373711.377673097\n",
      "Loss: 3279849.190243229\n",
      "Loss: 3188373.187698886\n",
      "Loss: 3099004.5220522434\n",
      "Loss: 3011576.6705322918\n",
      "Loss: 2925975.825762341\n",
      "Loss: 2842116.860895909\n",
      "Loss: 2759932.0625771126\n",
      "Loss: 2679365.2447879845\n",
      "Loss: 2600368.405628545\n",
      "Loss: 2522899.69415057\n",
      "Loss: 2446922.099301396\n",
      "Loss: 2372402.559278876\n",
      "Loss: 2299311.32707851\n",
      "Loss: 2227621.4984267238\n",
      "Loss: 2157308.6463310504\n",
      "Loss: 2088350.5279647997\n",
      "Loss: 2020726.8422087042\n",
      "Loss: 1954419.0238060467\n",
      "Loss: 1889410.064837544\n",
      "Loss: 1825684.3572442362\n",
      "Loss: 1763227.5520851463\n",
      "Loss: 1702026.4325049256\n",
      "Loss: 1642068.7982441776\n",
      "Loss: 1583343.3601010654\n",
      "Loss: 1525839.6431422466\n",
      "Loss: 1469547.8977259903\n",
      "Loss: 1414459.0175813136\n",
      "Loss: 1360564.4643112794\n",
      "Loss: 1307856.197774905\n",
      "Loss: 1256326.611862708\n",
      "Loss: 1205968.475224461\n",
      "Loss: 1156774.8765400287\n",
      "Loss: 1108739.1739491473\n",
      "Loss: 1061854.9482762637\n",
      "Loss: 1016115.9597039843\n",
      "Loss: 971516.1075641316\n",
      "Loss: 928049.392929902\n",
      "Loss: 885709.8837061289\n",
      "Loss: 844491.6819277409\n",
      "Loss: 804388.8929890348\n",
      "Loss: 765395.5965382867\n",
      "Loss: 727505.8187836274\n",
      "Loss: 690713.5059667242\n",
      "Loss: 655012.4987706692\n",
      "Loss: 620396.5074374337\n",
      "Loss: 586859.0873782269\n",
      "Loss: 554393.6150670536\n",
      "Loss: 522993.26401361567\n",
      "Loss: 492650.98061649274\n",
      "Loss: 463359.4597011399\n",
      "Loss: 435111.11954985606\n",
      "Loss: 407898.07623237855\n",
      "Loss: 381712.1170464134\n",
      "Loss: 356544.6728772232\n",
      "Loss: 332386.7892846643\n",
      "Loss: 309229.09612501075\n",
      "Loss: 287061.7755139263\n",
      "Loss: 265874.52793654107\n",
      "Loss: 245656.53631137992\n",
      "Loss: 226396.42781775442\n",
      "Loss: 208082.23330232326\n",
      "Loss: 190701.34409113953\n",
      "Loss: 174240.46605073623\n",
      "Loss: 158685.57076784506\n",
      "Loss: 144021.84375532373\n",
      "Loss: 130233.62964576723\n",
      "Loss: 117304.37440880711\n",
      "Loss: 105216.56472954933\n",
      "Loss: 93951.66482141003\n",
      "Loss: 83490.05112603494\n",
      "Loss: 73810.94558699602\n",
      "Loss: 64892.3484854831\n",
      "Loss: 56710.97220971935\n",
      "Loss: 49242.177811100395\n",
      "Loss: 42459.91679454435\n",
      "Loss: 36336.68131172698\n",
      "Loss: 30843.466780967083\n",
      "Loss: 25949.75194250692\n",
      "Loss: 21623.50244844154\n",
      "Loss: 17831.20522690381\n",
      "Loss: 14537.941947788106\n",
      "Loss: 11707.510785858061\n",
      "Loss: 9302.606078604904\n",
      "Loss: 7285.065070668375\n",
      "Loss: 5616.189297506345\n",
      "Loss: 4257.144813450431\n",
      "Loss: 3169.439979387003\n",
      "Loss: 2315.4716561785535\n",
      "Loss: 1659.12058481389\n",
      "Loss: 1166.3653302004457\n",
      "Loss: 805.8731461808817\n",
      "Loss: 549.5180582113978\n",
      "Loss: 372.77438809702363\n",
      "Loss: 254.9405005736876\n",
      "Loss: 179.16380721881117\n",
      "Loss: 132.2624632233866\n",
      "Loss: 104.36722817215993\n",
      "Loss: 88.43208193998555\n",
      "Loss: 79.67788299896833\n",
      "Loss: 75.03563029814744\n",
      "Loss: 72.64489251448325\n",
      "Loss: 71.44290918735068\n",
      "Loss: 70.85720230910381\n",
      "Loss: 70.5953810513992\n",
      "Loss: 70.51388304048373\n",
      "Loss: 70.54326403567094\n",
      "Stage=1, Model=2, 10032 data points\n",
      "Loss: 8666982.955547055\n",
      "Loss: 2350318.0295986934\n",
      "Loss: 463885.12673740817\n",
      "Loss: 63707.2458762056\n",
      "Loss: 5946.6281626484915\n",
      "Loss: 373.47990883579854\n",
      "Loss: 20.785500313198472\n",
      "Loss: 7.046080241255292\n",
      "Loss: 6.873355975720241\n",
      "Loss: 6.913705981543027\n",
      "Stage=1, Model=3, 10063 data points\n",
      "Loss: 9192413.650190582\n",
      "Loss: 6020665.085424914\n",
      "Loss: 3788888.890715855\n",
      "Loss: 2270667.342651093\n",
      "Loss: 1285841.012111661\n",
      "Loss: 682701.6112411799\n",
      "Loss: 337187.82841874764\n",
      "Loss: 153712.78676248738\n",
      "Loss: 64178.34582448667\n",
      "Loss: 24355.0880007224\n",
      "Loss: 8337.631762829938\n",
      "Loss: 2557.506340209985\n",
      "Loss: 701.8065576699414\n",
      "Loss: 176.9729385021317\n",
      "Loss: 47.9231456934801\n",
      "Loss: 20.93224245944203\n",
      "Loss: 16.367829980735788\n",
      "Loss: 15.854504291288636\n",
      "Loss: 15.878505435639287\n",
      "Stage=1, Model=4, 10142 data points\n",
      "Loss: 8613515.00273352\n",
      "Loss: 2810452.103715607\n",
      "Loss: 712441.9663915692\n",
      "Loss: 135008.75773338092\n",
      "Loss: 18657.26290604506\n",
      "Loss: 1839.8408217591814\n",
      "Loss: 129.60446409644626\n",
      "Loss: 13.821173626593586\n",
      "Loss: 10.295953789819134\n",
      "Loss: 10.79395077895687\n",
      "Stage=1, Model=5, 9960 data points\n",
      "Loss: 19086239.453699734\n",
      "Loss: 10977361.14447342\n",
      "Loss: 5884061.655445491\n",
      "Loss: 2900126.332698277\n",
      "Loss: 1298334.6799532888\n",
      "Loss: 521916.88615079434\n",
      "Loss: 186430.75860634385\n",
      "Loss: 58620.41267187532\n",
      "Loss: 16090.784687994626\n",
      "Loss: 3833.1545522257306\n",
      "Loss: 797.2143561330817\n",
      "Loss: 155.41835245603986\n",
      "Loss: 39.86601365651464\n",
      "Loss: 21.87922688191553\n",
      "Loss: 19.29488167826971\n",
      "Loss: 18.90012828722969\n",
      "Loss: 18.835865069566605\n",
      "Loss: 18.8365808080256\n",
      "Stage=1, Model=6, 9867 data points\n",
      "Loss: 29097668.872176163\n",
      "Loss: 4376038.383510191\n",
      "Loss: 367041.38397387107\n",
      "Loss: 16754.01301678055\n",
      "Loss: 488.0382956650539\n",
      "Loss: 49.44426674806685\n",
      "Loss: 37.938678482295245\n",
      "Loss: 38.5816866493881\n",
      "Stage=1, Model=7, 9924 data points\n",
      "Loss: 83645254.27256727\n",
      "Loss: 43993690.403360926\n",
      "Loss: 21043070.681144066\n",
      "Loss: 8998619.38032915\n",
      "Loss: 3388191.6995224417\n",
      "Loss: 1108399.003733309\n",
      "Loss: 311470.2127716486\n",
      "Loss: 74447.7755878875\n",
      "Loss: 15014.910483277476\n",
      "Loss: 2554.0964600632683\n",
      "Loss: 383.6736857323377\n",
      "Loss: 68.9707594822831\n",
      "Loss: 29.62017272597789\n",
      "Loss: 24.819500012715128\n",
      "Loss: 24.17890300773053\n",
      "Loss: 24.18376089942669\n",
      "Stage=1, Model=8, 10048 data points\n",
      "Loss: 25646850.49062459\n",
      "Loss: 3046921.580525995\n",
      "Loss: 179657.82919711227\n",
      "Loss: 4988.282030111065\n",
      "Loss: 61.89686394110235\n",
      "Loss: 9.81990781879959\n",
      "Loss: 10.965043187435542\n",
      "Stage=1, Model=9, 10012 data points\n",
      "Loss: 103959059.4889304\n",
      "Loss: 51435979.74663416\n",
      "Loss: 22723029.136224493\n",
      "Loss: 8787728.508558415\n",
      "Loss: 2924224.079146025\n",
      "Loss: 825151.2919975152\n",
      "Loss: 194962.49741131233\n",
      "Loss: 38091.8641059077\n",
      "Loss: 6067.966171153592\n",
      "Loss: 782.4634845100658\n",
      "Loss: 95.32483551592114\n",
      "Loss: 30.20345342607874\n",
      "Loss: 27.565980068881462\n",
      "Loss: 28.381818258039054\n"
     ]
    }
   ],
   "source": [
    "for stage_idx in range(num_stage):\n",
    "    models.append([])\n",
    "    \n",
    "    # if it's the last stage, we don't need to prepare datasets for the\n",
    "    # next stage. or, we need to initialize datasets for the next stage\n",
    "    if stage_idx != num_stage - 1:\n",
    "        next_data = [[] for i in range(num_model[stage_idx + 1])]\n",
    "        next_pos = [[] for i in range(num_model[stage_idx + 1])]\n",
    "\n",
    "    for model_idx in range(num_model[stage_idx]):\n",
    "\n",
    "        # initialize a model\n",
    "        model = Network(model_params[stage_idx][0], model_params[stage_idx][1]).to(\n",
    "            device\n",
    "        )\n",
    "        # use MSE loss for training\n",
    "        criterion = nn.MSELoss()\n",
    "        #criterion = nn.L1Loss()\n",
    "        # use Adam algo for training\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        #optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        # load dataset\n",
    "        data_gen = torch_data.DataLoader(\n",
    "            ds[stage_idx][model_idx], batch_size=batch_size, shuffle=False\n",
    "        )\n",
    "        # we will stop training when loss stops decreasing\n",
    "        last_loss = float(\"inf\")\n",
    "\n",
    "        print(\n",
    "            \"Stage={}, Model={}, {} data points\".format(\n",
    "                stage_idx, model_idx, len(ds[stage_idx][model_idx])\n",
    "            )\n",
    "        )\n",
    "        for epoch in range(epochs + 1):\n",
    "\n",
    "            # train model\n",
    "            for local_data, local_pos in data_gen:\n",
    "                local_data, local_pos = local_data.to(device), local_pos.to(device)\n",
    "                # feedforward\n",
    "                outputs = model(local_data)\n",
    "                # calc loss\n",
    "                loss = criterion(outputs, local_pos)\n",
    "                # back propagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            # calculate and print loss for this model at this epoch\n",
    "            with torch.set_grad_enabled(False):\n",
    "                loss_tot = 0.0\n",
    "                for local_data, local_pos in data_gen:\n",
    "                    local_data, local_pos = local_data.to(device), local_pos.to(device)\n",
    "                    outputs = model(local_data)\n",
    "                    loss = criterion(outputs, local_pos)\n",
    "                    loss_tot += loss.item()\n",
    "                   \n",
    "\n",
    "                print(\"Loss:\", loss_tot / len(ds[stage_idx][model_idx]))\n",
    "                #print(\"Loss:{}, last_loss={}, loss_tot={}, epoch={}\"\n",
    "                #      .format(loss_tot / len(ds[stage_idx][model_idx]), last_loss, loss_tot, epoch))\n",
    "                # if lost stops decreasing, just stop training\n",
    "\n",
    "#                 if last_loss == 0:\n",
    "#                     break\n",
    "                    \n",
    "#                 if last_loss < 0.001:\n",
    "#                     break\n",
    "                    \n",
    "#                if (loss_tot / len(ds[stage_idx][model_idx]) < 100) or epoch > 2000:\n",
    "                if (last_loss - loss_tot) / last_loss < 0.001:\n",
    "                    break\n",
    "                else:\n",
    "                    last_loss = loss_tot\n",
    "\n",
    " \n",
    "        # append the model we just trained to model tree\n",
    "        models[stage_idx].append(model)\n",
    "\n",
    "        # prepare datasets for the next stage. only when we're not at the last stage\n",
    "        if stage_idx != num_stage - 1:\n",
    "            \n",
    "            # load the datapoints in current set one by one. we need to assign each of them to a\n",
    "            # model in the next stage\n",
    "            data_gen = torch_data.DataLoader(\n",
    "                ds[stage_idx][model_idx], batch_size=1, shuffle=False\n",
    "            )\n",
    "            \n",
    "            for local_data, local_pos in data_gen:\n",
    "                local_data, local_pos = local_data.to(device), local_pos.to(device)\n",
    "                \n",
    "                # calculate which model in the next stage to assign to\n",
    "                # model_idx = output * num_model_next_stage / max_position\n",
    "                \n",
    "                output = model(local_data)\n",
    "                \n",
    "                model_sel = int(output.item() * num_model[stage_idx + 1] / max_pos)\n",
    "                \n",
    "                if model_sel >= num_model[stage_idx + 1]:\n",
    "                    model_sel = num_model[stage_idx + 1] - 1\n",
    "                elif model_sel <= 0:\n",
    "                    model_sel = 0\n",
    "                # append this datapoint to corresponding dataset\n",
    "                next_data[model_sel].append(local_data)\n",
    "                next_pos[model_sel].append(local_pos)\n",
    "\n",
    "    # create the Dataset objects for the next stage\n",
    "    if stage_idx != num_stage - 1:\n",
    "        ds.append([])\n",
    "        for next_model_idx in range(num_model[stage_idx + 1]):\n",
    "            ds[stage_idx + 1].append(\n",
    "                Dataset(next_data[next_model_idx], next_pos[next_model_idx])\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss: 626.5601187530925\n"
     ]
    }
   ],
   "source": [
    "# testing. load datapoints one by one\n",
    "test_ds = Dataset(data_list, pos_list)\n",
    "#test_ds = Dataset(pos_list, data_list)\n",
    "test_gen = torch_data.DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "err_tot = 0\n",
    "for local_data, local_pos in test_gen:\n",
    "    local_data, local_pos = local_data.to(device), local_pos.to(device)\n",
    "    model_sel = 0\n",
    "    for stage_idx in range(num_stage):\n",
    "        model = models[stage_idx][model_sel]\n",
    "        output = model(local_data)\n",
    "        # if it's not the last stage, the output determines which model\n",
    "        # in the next stage to use\n",
    "        if stage_idx != num_stage - 1:\n",
    "            model_sel = int(output.item() * num_model[stage_idx + 1] / max_pos)\n",
    "            if model_sel >= num_model[stage_idx + 1]:\n",
    "                model_sel = num_model[stage_idx + 1] - 1\n",
    "            elif model_sel <= 0:\n",
    "                model_sel = 0\n",
    "        # if it's the last layer, the output is the position (index)\n",
    "        else:\n",
    "            err_tot += abs(int(output.item()) - int(local_pos.item()))\n",
    "\n",
    "print(\"Final Loss:\", float(err_tot) / len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
