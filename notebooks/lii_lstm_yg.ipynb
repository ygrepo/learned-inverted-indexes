{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import external dependencies\n",
    "import sys\n",
    "from IPython import display\n",
    "from ipywidgets import Output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import local dependencies\n",
    "sys.path.insert(0, \"../src\")\n",
    "from collection import Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Set up device and manual seed\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "test_collection = Collection(\"../test_data/test_collection\")\n",
    "posting_lists = []\n",
    "posting_length_to_use = 128\n",
    "for _, pl in enumerate(test_collection):\n",
    "    if len(pl[0]) >= posting_length_to_use:\n",
    "        posting_lists.append(np.array(pl[0], dtype=np.int32))\n",
    "posting_lists.sort(key=lambda x:np.shape(x)[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest seq: 9366\n",
      "Shortest seq: 128\n",
      "Average seq: 699.22\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(pl) for pl in posting_lists]\n",
    "print(\"Longest seq: {}\".format(max(lengths)))\n",
    "print(\"Shortest seq: {}\".format(min(lengths)))\n",
    "print(\"Average seq: {:.2f}\".format(np.array(lengths).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_tensors(data):\n",
    "    tensor_list = []\n",
    "    for pl in data:\n",
    "        tensor_list.append(torch.tensor(pl, dtype=torch.float32))\n",
    "    return tensor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list_of_tensors(posting_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, source_lengths, i, bsz):\n",
    "    batch_size = min(bsz, len(source_lengths))\n",
    "    return source[i:i+batch_size], source_lengths[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(source, i, bptt):\n",
    "    data = [d.unsqueeze(dim=1).to(device) for d in source]\n",
    "    target = [d.to(device) for d in source]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, i in enumerate(range(0, len(lengths), 3)):\n",
    "#     print(batch_idx, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, rnn_type, ninp, nhid, nlayers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, batch_first=False)\n",
    "        else:\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, batch_first=False)\n",
    "        self.linear = nn.Linear(nhid, 1)  # go from hidden dim to dim of 1\n",
    "            \n",
    "    def forward(self, input_seqs, input_lengths, hidden):\n",
    "        input_padded = nn.utils.rnn.pad_sequence(input_seqs, padding_value=0.0, batch_first=False)\n",
    "        input_packed = nn.utils.rnn.pack_padded_sequence(input_padded, input_lengths, batch_first=False)\n",
    "        output_packed, hidden = self.rnn(input_packed, hidden)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(output_packed, padding_value=0.0, batch_first=False)\n",
    "        batch_size, max_seq_len, _ = output.size()\n",
    "        output = output.contiguous()\n",
    "        output = output.view(-1, output.shape[2])\n",
    "        output = self.linear(output)\n",
    "        output = output.view(max_seq_len, batch_size, 1)\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "        else:\n",
    "            return weight.new_zeros(self.nlayers, bsz, self.nhid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, posting_lists, lengths,\n",
    "          scheduler=None, epochs=2000, batch_size=3, bptt=10000, log_interval=10, plots=False):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    current_epoch_progress = Output()\n",
    "    display.display(current_epoch_progress)\n",
    "    total_epoch_progress = Output()\n",
    "    display.display(total_epoch_progress)\n",
    "    loss_plot = Output()\n",
    "    display.display(loss_plot)\n",
    "    \n",
    "    # Loop for number of epochs:\n",
    "    for e in range(1, epochs+1):\n",
    "        with total_epoch_progress:\n",
    "            print(\"Epoch {}/{} [{:.2f}%]\".format(e, epochs, (e/epochs)*100))\n",
    "        total_epoch_progress.clear_output(wait=True)\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # Loop for batches within data:\n",
    "        for batch_idx, i in enumerate(range(0, len(lengths), batch_size)):\n",
    "            batch, batch_lengths = get_batch(posting_lists, lengths, i, batch_size)\n",
    "            with current_epoch_progress:\n",
    "                print(\"Current Epoch {}: {}/{} [{:.2f}%]\".format(e, \n",
    "                                                         i+batch_size, \n",
    "                                                         len(lengths), \n",
    "                                                         ((i+batch_size)/len(lengths))*100))\n",
    "            current_epoch_progress.clear_output(wait=True)\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "            \n",
    "            # Get data\n",
    "            data, target = get_data(batch, i, bptt)\n",
    "\n",
    "            # Zero out the grad\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            # Get output\n",
    "            prediction = model(data, batch_lengths, hidden)\n",
    "                \n",
    "            # Calculate loss\n",
    "            target = nn.utils.rnn.pad_sequence(target, padding_value=0.0, batch_first=False)\n",
    "            loss = F.mse_loss(prediction, target)\n",
    "            epoch_loss += loss.item()\n",
    "                \n",
    "            # Take gradient step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Take scheduler step\n",
    "            if scheduler:\n",
    "                scheduler.step(loss)\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # Print loss and plot predicitons vs. ground truth\n",
    "        if e % log_interval == 0:\n",
    "            with loss_plot:\n",
    "                plt.plot(list(range(len(epoch_losses))), epoch_losses)\n",
    "                plt.title(\"Loss per epoch\")\n",
    "                plt.xlabel(\"Epochs\")\n",
    "                plt.ylabel(\"MSE Loss\")\n",
    "                plt.show()\n",
    "            loss_plot.clear_output(wait=True)\n",
    "            print(\"Train Epoch {}: Loss - {}, Avg Loss - {}\".format(e, epoch_losses[-1], sum(epoch_losses)/(e+1)))\n",
    "            if plots:\n",
    "                with torch.no_grad():\n",
    "                    plot_pred = prediction.cpu().detach().numpy().reshape(-1)\n",
    "                    plot_index = list(range(plot_pred.shape[0]))\n",
    "                    plot_target = target.cpu().numpy().reshape(-1)\n",
    "#                     error = target - prediction\n",
    "#                     max_error = abs(error.max().item())\n",
    "                    error = 0\n",
    "                    max_error = 0\n",
    "                    title = \"Train Epoch {}: Loss - {}; Max error - {}\".format(e, loss.item(), max_error)\n",
    "                    plt.plot(plot_index, plot_target, linestyle=\"None\", marker=\"o\", label=\"True\")\n",
    "                    plt.plot(plot_index, plot_pred, linestyle=\"None\", marker=\"o\", label=\"Prediction\")\n",
    "                    plt.title(title)\n",
    "                    plt.legend(loc=\"best\")\n",
    "                    plt.show()\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-1d92d76a0591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m274\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                     plots=False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-61ac15bec47b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, posting_lists, lengths, scheduler, epochs, batch_size, bptt, log_interval, plots)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# Take gradient step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "rnn_type = \"GRU\"\n",
    "input_size = 1\n",
    "hidden_size = 10\n",
    "layers = 1\n",
    "lii_rnn = RNNModel(rnn_type, input_size, hidden_size, layers)\n",
    "lii_rnn.to(device)\n",
    "optimizer = optim.Adam(params=lii_rnn.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=20, verbose=True, threshold=10e-6)\n",
    "predictions = train(lii_rnn,\n",
    "                    optimizer,\n",
    "                    data,\n",
    "                    lengths,\n",
    "                    scheduler,\n",
    "                    epochs=1000,\n",
    "                    batch_size=274,\n",
    "                    log_interval=10,\n",
    "                    plots=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lii_lstm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2507624ff7a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model's state_dict:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mparam_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlii_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlii_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam_tensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#torch.save(lii_lstm.state_dict(), \"/Users/yairschiff/Development/Python/DeepLearning/Project/test_lstm.pth\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lii_lstm' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in lii_lstm.state_dict():\n",
    "    print(param_tensor, \"\\t\", lii_lstm.state_dict()[param_tensor].size())\n",
    "    \n",
    "#torch.save(lii_lstm.state_dict(), \"/Users/yairschiff/Development/Python/DeepLearning/Project/test_lstm.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare prediction and ground truth\n",
    "for i in range(predictions.size()[0]):\n",
    "    print(\"True value: {} vs. Predicted: {}\".format(int((posting_lists[0][i])), int((predictions[i].item()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather and print metrics\n",
    "# columns = ['MSE', 'R2 score', 'MAE', 'Expl.Var']\n",
    "# rows = ['Performance']\n",
    "# metrics = pd.DataFrame(0.0, columns=columns, index=rows)\n",
    "# prediction_values = prediction_scaled.detach().numpy().reshape(-1)\n",
    "# inverted_index_values = inverted_index_scaled.detach().numpy().reshape(-1)\n",
    "# metrics.iloc[0,0] = np.sqrt(mean_squared_error(prediction_values, inverted_index_values))\n",
    "# metrics.iloc[0,1] = r2_score(prediction_values, inverted_index_values)\n",
    "# metrics.iloc[0,2] = mean_absolute_error(prediction_values, inverted_index_values)\n",
    "# metrics.iloc[0,3] = explained_variance_score(prediction_values, inverted_index_values)\n",
    "# print(metrics)\n",
    "# print('Number of incorrect indexes (Mismatched) ={}, for {} keys'\n",
    "#       .format(np.count_nonzero(prediction_values != inverted_index_values), inverted_index_values.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create tensor data\n",
    "# data = np.zeros((total_seq_length, 2))\n",
    "# current_row = 0\n",
    "# for i in range(len(posting_lists)):\n",
    "#     idxs = np.array([i]*len(posting_lists[i]))\n",
    "#     data[current_row:current_row+len(posting_lists[i])] = np.vstack((idxs, posting_lists[i])).T\n",
    "#     current_row += len(posting_lists[i])\n",
    "# data_tensor = torch.tensor(data, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def masked_loss(prediction, target):\n",
    "#     prediction = prediction.squeeze()\n",
    "#     zeros = torch.zeros_like(prediction)\n",
    "#     masked_prediction = torch.where(prediction == -1, zeros, prediction)\n",
    "#     masked_target = torch.where(prediction == -1, zeros, target)\n",
    "#     return F.mse_loss(masked_prediction, masked_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
