{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import external dependencies\n",
    "import pdb\n",
    "import os\n",
    "import sys\n",
    "from IPython import display\n",
    "from ipywidgets import Output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from itertools import accumulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import local dependencies\n",
    "sys.path.insert(0, \"../src\")\n",
    "from collection import Collection\n",
    "from global_model import RNNModel, LSTMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up device and manual seed\n",
    "torch.manual_seed(1)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up params\n",
    "MODE = \"index2doc\"  # options: \"doc2doc\" (predict next doc id from previous) or \"index2doc\" (predict from index)\n",
    "RNN_TYPE = \"LSTM\"  # options: LSTM, GRU, RNN_TANH, RNN_RELU, LSTMAE\n",
    "EMBED = True  # set this flag if you want to concatenate term id to input  \n",
    "SCALE = False  # set this flag to scale doc ids [0, 1] (by dividing by max doc id)\n",
    "LOSS = \"L1\"  # options: \"L1\" or \"L2\"\n",
    "THRESHOLD = 128  # minimum length posting list to use\n",
    "INPUT_SIZE = 2 if EMBED else 1\n",
    "HIDDEN_SIZE = 100\n",
    "LAYERS = 5\n",
    "EPOCHS = 10000\n",
    "BATCH_SIZE = 247\n",
    "LOG_INTERVAL = 1000\n",
    "print(\"Running: {} {} with {} and {} loss \\\n",
    "      \\n\\t- posting list threshold: {} \\\n",
    "      \\n\\t- input size: {} \\\n",
    "      \\n\\t- hidden units: {} \\\n",
    "      \\n\\t- layers: {} \\\n",
    "      \\n\\t- batch size: {} \\\n",
    "      \\n\\t- epochs: {}\".format(MODE, \n",
    "                               RNN_TYPE, \n",
    "                               \"scaling\" if SCALE else \"no scaling\",\n",
    "                               LOSS,\n",
    "                               THRESHOLD,\n",
    "                               INPUT_SIZE,\n",
    "                               HIDDEN_SIZE,\n",
    "                               LAYERS,\n",
    "                               BATCH_SIZE,\n",
    "                               EPOCHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "test_collection = Collection(\"../test_data/test_collection\")\n",
    "posting_lists = []\n",
    "posting_length_to_use = THRESHOLD\n",
    "for term_id, pl in enumerate(test_collection):\n",
    "    if len(pl[0]) >= posting_length_to_use:\n",
    "        posting_lists.append((np.array(pl[0], dtype=np.int32), term_id))\n",
    "posting_lists.sort(key=lambda x:np.shape(x[0])[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather length and max doc id info\n",
    "lengths = [len(pl[0]) for pl in posting_lists]\n",
    "max_doc_id = max([pl[0].max() for pl in posting_lists])\n",
    "max_term_id = float(max(pl[1] for pl in posting_lists))\n",
    "scale_factor = float(max_doc_id) if SCALE else 1.0\n",
    "print(\"Number of seqs: {}\".format(len(lengths)))\n",
    "print(\"Longest seq: {}\".format(max(lengths)))\n",
    "print(\"Shortest seq: {}\".format(min(lengths)))\n",
    "print(\"Average seq: {:.2f}\".format(np.array(lengths).mean()))\n",
    "print(\"Max doc id: {}\".format(max_doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(input_seq, term_id):\n",
    "    embedding = input_seq.unsqueeze(dim=1).repeat(1, 2)\n",
    "    embedding.index_fill_(1, torch.tensor([1]), term_id)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_tensors(device, source, mode=\"doc2doc\", embedding=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for pl in source:\n",
    "        if mode == \"doc2doc\":\n",
    "            if embedding:\n",
    "                # Scale term id from 0 to 1\n",
    "                data.append(embed(torch.tensor(pl[0], dtype=torch.float32) / scale_factor, \n",
    "                                  pl[1] / max_term_id).to(device))\n",
    "            else:\n",
    "                data.append(torch.tensor(pl[0], dtype=torch.float32).unsqueeze(dim=1).to(device))\n",
    "        else:\n",
    "            if embedding:\n",
    "                # Scale term id from 0 to 1\n",
    "                data.append(embed(torch.arange(pl[0].size, dtype=torch.float32), pl[1] / max_term_id).to(device))\n",
    "            else:\n",
    "                data.append(torch.arange(pl[0].size, dtype=torch.float32).unsqueeze(dim=1).to(device))\n",
    "        labels.append((torch.tensor(pl[0], dtype=torch.float32) / scale_factor).to(device))\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data, source_labels = list_of_tensors(DEVICE, posting_lists, MODE, EMBED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(source_data[2224].shape)\n",
    "print(source_data[2224])\n",
    "print(source_labels[2224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, target, source_lengths, i, bsz):\n",
    "    batch_size = min(bsz, len(source_lengths))\n",
    "    return data[i:i+batch_size], target[i:i+batch_size], source_lengths[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data, target, mode=\"doc2doc\"):\n",
    "    if mode == \"doc2doc\":\n",
    "        data = [d[:-1] for d in data]\n",
    "        target = [t[1:] for t in target]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, source_data, source_labels, lengths,\n",
    "          mode=\"doc2doc\", scheduler=None, epochs=2000, batch_size=3, log_interval=10):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    total_epoch_progress = Output()\n",
    "    display.display(total_epoch_progress)\n",
    "    current_epoch_progress = Output()\n",
    "    display.display(current_epoch_progress)\n",
    "    loss_plot = Output()\n",
    "    display.display(loss_plot)\n",
    "    \n",
    "    # Wrap single posting lists as lists\n",
    "    if not isinstance(source_data, list):\n",
    "        source_data = [source_data]\n",
    "        source_labels = [source_labels]\n",
    "        lengths = [lengths]\n",
    "    \n",
    "    # Loop for number of epochs:\n",
    "    for e in range(1, epochs+1):\n",
    "        with total_epoch_progress:\n",
    "            print(\"Epoch {}/{} [{:.2f}%]\".format(e, epochs, (e/epochs)*100))\n",
    "        total_epoch_progress.clear_output(wait=True)\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # Loop for batches within data:\n",
    "        for batch_idx, i in enumerate(range(0, len(lengths), batch_size)):\n",
    "            batch_data, batch_target, batch_lengths = get_batch(source_data, \n",
    "                                                                source_labels, lengths, i, batch_size)\n",
    "            with current_epoch_progress:\n",
    "                print(\"Current Epoch {}: {}/{} [{:.2f}%]\".format(e, \n",
    "                                                         i+min(batch_size, len(batch_data)), \n",
    "                                                         len(lengths), \n",
    "                                                         ((i+min(batch_size, len(batch_data)))/len(lengths))*100))\n",
    "            current_epoch_progress.clear_output(wait=True)\n",
    "#             hidden = model.init_hidden(min(batch_size, len(batch_data)))\n",
    "\n",
    "            # Get data\n",
    "            data, target = get_data(batch_data, batch_target, mode)\n",
    "\n",
    "            # Zero out the grad\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            # Get output\n",
    "#             prediction, _ = model(data, batch_lengths, hidden)\n",
    "            prediction, _ = model(data, batch_lengths)\n",
    "\n",
    "            # Calculate loss\n",
    "            target = nn.utils.rnn.pad_sequence(target, padding_value=0.0, batch_first=False)\n",
    "            if prediction.dim() == 1:\n",
    "                target = target.squeeze()\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item()\n",
    "                \n",
    "            # Take gradient step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Take scheduler step\n",
    "            if scheduler:\n",
    "                scheduler.step(loss)\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # Print loss and plot predicitons vs. ground truth\n",
    "        if e % log_interval == 0:\n",
    "            with loss_plot:\n",
    "                plt.plot(list(range(len(epoch_losses))), epoch_losses)\n",
    "                plt.title(\"Loss per epoch\")\n",
    "                plt.xlabel(\"Epochs\")\n",
    "                plt.ylabel(\"Loss\")\n",
    "                plt.show()\n",
    "            loss_plot.clear_output(wait=True)\n",
    "            print(\"Train Epoch {}: Loss - {}, Avg Loss - {}\".format(e, epoch_losses[-1], sum(epoch_losses)/(e+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model and setup training\n",
    "if RNN_TYPE == \"LSTMAE\":\n",
    "    lii_rnn = LSTMAE(MODE, INPUT_SIZE, HIDDEN_SIZE, LAYERS)\n",
    "else:\n",
    "    lii_rnn = RNNModel(MODE, RNN_TYPE, INPUT_SIZE, HIDDEN_SIZE, LAYERS)\n",
    "lii_rnn.to(DEVICE)\n",
    "optimizer = optim.Adam(params=lii_rnn.parameters(), lr=0.1)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=20, verbose=True, threshold=10e-6)\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "train(model=lii_rnn,\n",
    "      criterion=nn.L1Loss() if LOSS == \"L1\" else nn.MSELoss(),\n",
    "      optimizer=optimizer,\n",
    "      source_data=source_data[2224],\n",
    "      source_labels=source_labels[2224],\n",
    "      lengths=lengths[2224],\n",
    "      scheduler=scheduler,\n",
    "      mode=MODE,\n",
    "      epochs=EPOCHS,\n",
    "      batch_size=BATCH_SIZE,\n",
    "      log_interval=LOG_INTERVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_doc2doc(model, posting_lists, lengths, primer_tokens=1, embedding=False, scaler=1):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    prediction_progress = Output()\n",
    "    display.display(prediction_progress)\n",
    "    pl_progress = Output()\n",
    "    display.display(pl_progress)\n",
    "    \n",
    "    # Wrap single posting lists as lists\n",
    "    if not isinstance(posting_lists, list):\n",
    "        posting_lists = [posting_lists]\n",
    "        lengths = [lengths]\n",
    "    \n",
    "    for idx, pl in enumerate(posting_lists):\n",
    "        with prediction_progress:\n",
    "            print(\"Prediction Progres {}/{} [{:.2f}%]\".format(idx+1,\n",
    "                                                              len(posting_lists),\n",
    "                                                              ((idx+1)/len(posting_lists))*100))\n",
    "        prediction_progress.clear_output(wait=True)\n",
    "        if embedding:\n",
    "            term_id = pl[0, 1]\n",
    "        hidden = model.init_hidden(1)\n",
    "        # Get first output\n",
    "        prediction, hidden = model([pl[:primer_tokens]], [primer_tokens+1], hidden)\n",
    "        if not embedding:\n",
    "            prediction = prediction.unsqueeze(dim=0) * scale_factor\n",
    "        next_prediction = prediction\n",
    "        for i in range(lengths[idx] - primer_tokens):\n",
    "            with pl_progress:\n",
    "                print(\"PL {} Progres {}/{} [{:.2f}%]\".format(idx+1,\n",
    "                                                             i+1+primer_tokens,\n",
    "                                                             lengths[idx],\n",
    "                                                             ((i+1+primer_tokens)/lengths[idx])*100))\n",
    "            pl_progress.clear_output(wait=True)\n",
    "            if embedding:\n",
    "                next_token = embed(next_prediction.squeeze(), term_id)\n",
    "            else:\n",
    "                next_token = next_prediction.unsqueeze(dim=0)\n",
    "            next_prediction, hidden = model([next_token], [2], hidden)\n",
    "            if not embedding:\n",
    "                next_prediction = next_prediction.unsqueeze(dim=0) * scale_factor\n",
    "            prediction = torch.cat((prediction, next_prediction))\n",
    "        predictions.append(prediction.squeeze())\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def evaluate_index2doc(model, indexes, lengths, scale_factor=1):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    prediction_progress = Output()\n",
    "    display.display(prediction_progress)\n",
    "    \n",
    "    # Wrap single posting lists as lists\n",
    "    if not isinstance(indexes, list):\n",
    "        indexes = [indexes]\n",
    "        lengths = [lengths]\n",
    "    \n",
    "    for i, pl in enumerate(indexes):\n",
    "        with prediction_progress:\n",
    "            print(\"Prediction Progres {}/{} [{:.2f}%]\".format(i+1,\n",
    "                                                              len(indexes),\n",
    "                                                              ((i+1)/len(indexes))*100))\n",
    "        prediction_progress.clear_output(wait=True)\n",
    "        hidden = model.init_hidden(1)\n",
    "        predictions.append(model([pl], [lengths[i]], hidden)[0].squeeze() * scale_factor)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "if MODE == \"doc2doc\":\n",
    "    predictions = evaluate_doc2doc(lii_rnn, source_data[2224], lengths[2224],\n",
    "                                   primer_tokens=1, scale_factor=scale_factor, embedding=EMBED)\n",
    "else:\n",
    "    predictions = evaluate_index2doc(lii_rnn, source_data[2224], lengths[2224], scale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(predictions[0].size(0)), source_labels[2224].cpu().detach().numpy(), label=\"Ground truth\")\n",
    "plt.plot(np.arange(predictions[0].size(0)), predictions[0].cpu().detach().numpy(), label=\"Predictions\")\n",
    "plt.xlabel(\"Posting list Index\")\n",
    "plt.ylabel(\"Doc id\")\n",
    "plt.title(\"Predicted vs. Ground truth Doc IDs\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "print(source_labels[2224])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in lii_rnn.state_dict():\n",
    "    logging.info(\"{} \\t {}\".format(param_tensor, lii_rnn.state_dict()[param_tensor].size()))\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"../models\")\n",
    "model_path = os.path.join(MODEL_DIR, \"{}_{}_h{}_l{}_e{}.pth\".format(mode, rnn_type, hidden_size, layers, epochs))\n",
    "print(\"Saving model to: {}\".format(model_path))\n",
    "torch.save(lii_rnn.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zigzag_encode (i):\n",
    "    return (i >> 31) ^ (i << 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save delta between prediction and target\n",
    "deltas_list = []\n",
    "global_max = 0\n",
    "for idx, tens in enumerate(predictions):\n",
    "    delta = (tens.round() - data[idx]).detach().tolist()\n",
    "    delta_zigzag = [data[idx].size(0)] + list(accumulate([zigzag_encode(int(i)) for i in delta]))\n",
    "    global_max = max(delta_zigzag[-1], global_max)\n",
    "    deltas_list.append(np.array(delta_zigzag, dtype=np.uint32))\n",
    "global_max_list = [1, global_max]\n",
    "deltas_array = np.concatenate([np.array(global_max_list, dtype=np.uint32)] + deltas_list).ravel()\n",
    "DELTA_DIR = os.path.join(BASE_DIR, \"../deltas\")\n",
    "delta_file = os.path.join(DELTA_DIR, \"{}_{}_h{}_l{}_e{}.docs\".format(mode, rnn_type, hidden_size, layers, epochs))\n",
    "print(\"Saving deltas to: {}\".format(delta_file))\n",
    "with open(delta_file, \"wb\") as binfile:\n",
    "    deltas_array.tofile(binfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
